{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-08-08T11:34:31.119968Z",
     "end_time": "2023-08-08T11:34:31.125557Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import optimizers\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelBinarizer, LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importation des données"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('Data/UNSW-NB15 - CSV Files/a part of training and testing set/UNSW_NB15_training-set.csv',\n",
    "                         low_memory=False)\n",
    "data_test = pd.read_csv('Data/UNSW-NB15 - CSV Files/a part of training and testing set/UNSW_NB15_testing-set.csv',\n",
    "                        low_memory=False)\n",
    "# x = dataset.iloc[:, 1:44].values\n",
    "# y = dataset.iloc[:, 44].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-08T09:44:19.664911Z",
     "end_time": "2023-08-08T09:44:20.508219Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prétraitement des données\n",
    "Cette fonction est conçue pour prétraiter les données en vue d'une utilisation avec des réseaux de neurones convolutionnels (CNN). Elle prend en entrée un tableau data et réalise les étapes suivantes :\n",
    "- Normalisation des features : Les caractéristiques sélectionnées sont converties en float et normalisé pour avoir des valeurs comprises entre 0 et 1. Cette normalisation est essentielle pour s'assurer que toutes les caractéristiques ont le même poids lors de l'entraînement du modèle.\n",
    "\n",
    "- Encodage des Étiquettes : Les étiquettes textuelles de l'ensemble de données sont d'abord transformées en étiquettes numériques à l'aide d'un LabelEncoder. Exemple : 'Normal' devient 0, 'Exploits' devient 1, etc. Ensuite, pour permettre une classification multiclasse, ces étiquettes numériques sont transformées en étiquettes binaires à l'aide d'un LabelBinarizer. Ce qui permet d'obtenir de nouvelles colonnes pour chaque étiquette, avec des valeurs binaires (0 ou 1) indiquant si l'échantillon appartient à cette classe ou non.\n",
    "\n",
    "- Adaptation à l'entrée CNN: Les CNN traitent des images en 3D, avec une hauteur, une largeur et des canaux (comme les couleurs RGB). Nos données sont en 1D, donc nous les transformons pour qu'elles ressemblent à de petites \"images\" 3D. Chaque échantillon devient une \"image\" avec la longueur des caractéristiques comme hauteur, une largeur de 1 et un canal unique. Ainsi, nous pouvons utiliser un CNN sur des données qui ne sont pas vraiment des images.\n",
    "\n",
    "La fonction renvoie finalement deux tableaux : x, qui contient les caractéristiques prétraitées et remodelées, et y, qui contient les étiquettes binarisées. La forme 3D de x est particulièrement adaptée pour être utilisée comme entrée dans un réseau de neurones convolutionnel."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    # Initialisation des outils de prétraitement\n",
    "    scaler = MinMaxScaler()  # Transformation des données en valeurs comprises entre 0 et 1\n",
    "    label_encoder = LabelEncoder()  # Transformation des étiquettes textuelles en étiquettes numériques\n",
    "    label_binarizer = LabelBinarizer()  # Transformation des étiquettes multiclasse en étiquettes binaires\n",
    "\n",
    "    # Sélection des caractéristiques spécifiées pour l'ensemble de test\n",
    "    x = data[:, [1, 6, 7, 8, 9, 10, 11, 12, 13, 27, 28, 32, 33, 34, 35, 36]]\n",
    "\n",
    "    # Conversion des données en flottants et normalisation\n",
    "    x = x.astype(float)\n",
    "    scaler.fit(x)  # Calcul des paramètres de normalisation\n",
    "    x = scaler.transform(x)  # Normalisation des données\n",
    "\n",
    "    # Récupération des étiquettes de l'ensemble de test\n",
    "    label = data[:, 43]  # 43 = attack_cat\n",
    "\n",
    "    # Encodage des étiquettes textuelles en étiquettes numériques\n",
    "    label_encoder.fit(label)\n",
    "    y = label_encoder.transform(label)\n",
    "\n",
    "    # Transformation des étiquettes numériques en étiquettes binaires\n",
    "    label_binarizer.fit(y)\n",
    "    y = label_binarizer.transform(y)\n",
    "\n",
    "    # Remodelage des données pour les adapter à l'entrée 3D attendue par CNN\n",
    "    x_final = []\n",
    "    size = np.size(x, axis=1)\n",
    "    for sample in x:\n",
    "        reshaped_sample = sample.reshape([size, 1, 1])\n",
    "        x_final.append(reshaped_sample)\n",
    "    x = np.array(x_final)\n",
    "\n",
    "    # La fonction renvoie les données prétraitées pour les caractéristiques et les étiquettes\n",
    "    return x, y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-08T10:19:09.935717Z",
     "end_time": "2023-08-08T10:19:09.937599Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "x_train, y_train = preprocess(data_train.values)\n",
    "x_test, y_test = preprocess(data_test.values)\n",
    "\n",
    "shape = np.size(x_train, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-08T10:39:44.152888Z",
     "end_time": "2023-08-08T10:39:44.847099Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "nb_classes = y_train.shape[1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-08T11:31:00.135671Z",
     "end_time": "2023-08-08T11:31:00.138500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Création du modèle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "model = Sequential()  # Initialisation du modèle\n",
    "\n",
    "### Bloc 1\n",
    "## Deux couches de convolution avec une fonction d'activation ReLU\n",
    "# 64 filtres de taille 3x1\n",
    "# input_shape = (16 caractéristiques, 1 hauteur, 1 canal)\n",
    "model.add(Conv2D(64, (3, 1), activation='relu', input_shape=(shape, 1, 1)))\n",
    "model.add(Conv2D(64, (3, 1), activation='relu'))\n",
    "\n",
    "## Couche de Pooling\n",
    "# Réduit la complexité du modèle en réduisant la taille des données\n",
    "# 2x1 filtre → réduit de 2 la hauteur et conserve la largeur\n",
    "model.add(MaxPooling2D(pool_size=(2, 1)))\n",
    "\n",
    "## Trois couches de convolution\n",
    "# 128 filtres de taille 3x1\n",
    "# padding=\"same\" → assure que la sortie à la même taille que l'entrée\n",
    "# ce qui permet d'éviter la perte d'information\n",
    "model.add(Conv2D(128, (3, 1), activation='relu'))\n",
    "model.add(Conv2D(128, (3, 1), activation='relu', padding=\"same\"))\n",
    "model.add(Conv2D(128, (3, 1), activation='relu', padding=\"same\"))\n",
    "\n",
    "## Couche de Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 1)))\n",
    "\n",
    "## Trois couches de convolution\n",
    "model.add(Conv2D(256, (3, 1), activation='relu', padding=\"same\"))\n",
    "model.add(Conv2D(256, (3, 1), activation='relu', padding=\"same\"))\n",
    "model.add(Conv2D(256, (3, 1), activation='relu', padding=\"same\"))\n",
    "\n",
    "## Couche de Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 1)))\n",
    "\n",
    "# Aplatir les données en un vecteur 1D (actuellement 3D)\n",
    "model.add(Flatten())\n",
    "\n",
    "### Bloc 2\n",
    "## Deux couches entièrement connectées\n",
    "# 100 neurones\n",
    "model.add(Dense(100, kernel_initializer='normal', activation='relu'))\n",
    "# Dropout → évite le surapprentissage\n",
    "# 50% des neurones sont ignorés aléatoirement à chaque itération, force le modèle à apprendre de nouvelles représentations\n",
    "model.add(Dropout(0.5))\n",
    "# name='output' → nomme la couche pour pouvoir y accéder plus tard\n",
    "model.add(Dense(20, kernel_initializer='normal', activation='relu', name='output'))\n",
    "\n",
    "# Couche de sortie\n",
    "# 10 neurones → 10 classes\n",
    "# softmax → fonction d'activation pour les problèmes de classification multiclasse\n",
    "model.add(Dense(nb_classes, kernel_initializer='normal', activation='softmax'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-08T11:31:06.914411Z",
     "end_time": "2023-08-08T11:31:07.293971Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compilation du modèle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "opt = optimizers.Adam()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['categorical_accuracy'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-08T11:34:50.483994Z",
     "end_time": "2023-08-08T11:34:50.548384Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Création du modèle (Tentative 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 40\u001B[0m\n\u001B[1;32m     33\u001B[0m classifier\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m'\u001B[39m, loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbinary_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m, metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# Entraînement du modèle\u001B[39;00m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# Supposons que `train_data` et `train_labels` soient vos données et labels d'entraînement\u001B[39;00m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m# et que `val_data` et `val_labels` soient vos données et labels de validation.\u001B[39;00m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;66;03m# epochs représente le nombre de fois où l'ensemble des données d'entraînement est utilisé pour l'apprentissage\u001B[39;00m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m# batch_size représente le nombre d'échantillons qui seront propagés à travers le réseau à la fois.\u001B[39;00m\n\u001B[0;32m---> 40\u001B[0m \u001B[43mclassifier\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_split\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/Network-intrusion-detection-with-machine-learning/venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/PycharmProjects/Network-intrusion-detection-with-machine-learning/venv/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:98\u001B[0m, in \u001B[0;36mconvert_to_eager_tensor\u001B[0;34m(value, ctx, dtype)\u001B[0m\n\u001B[1;32m     96\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m dtypes\u001B[38;5;241m.\u001B[39mas_dtype(dtype)\u001B[38;5;241m.\u001B[39mas_datatype_enum\n\u001B[1;32m     97\u001B[0m ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m---> 98\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEagerTensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mValueError\u001B[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
     ]
    }
   ],
   "source": [
    "classifier = Sequential()\n",
    "\n",
    "# Couche de convolution initiale\n",
    "# 32 filtres de taille 3x3\n",
    "# 'relu' est utilisé comme fonction d'activation pour ajouter de la non-linéarité\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape=(64, 64, 3), activation='relu'))\n",
    "\n",
    "# Couche de pooling pour réduire la dimensionnalité tout en conservant les caractéristiques\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Seconde couche de convolution\n",
    "# Utilise également 32 filtres de taille 3x3\n",
    "classifier.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "\n",
    "# Flattening: transforme le format 2D de la carte des caractéristiques en un vecteur 1D\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Première couche entièrement connectée (Dense)\n",
    "# 128 neurones, fonction d'activation 'relu'\n",
    "classifier.add(Dense(units=128, activation='relu'))\n",
    "\n",
    "# Dropout pour éviter le surapprentissage. Désactive 50% des neurones de manière aléatoire pendant chaque itération d'entraînement.\n",
    "classifier.add(Dropout(0.5))\n",
    "\n",
    "# Couche de sortie\n",
    "# Une seule unité pour une classification binaire avec fonction d'activation sigmoïde pour obtenir une probabilité\n",
    "classifier.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compilation du modèle\n",
    "# Utilisation de l'optimiseur 'adam' pour la descente de gradient\n",
    "# Utilisation de la fonction de perte 'binary_crossentropy' pour la classification binaire\n",
    "# Utilisation de la métrique 'accuracy' pour évaluer les performances du modèle\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entraînement du modèle\n",
    "# Supposons que `train_data` et `train_labels` soient vos données et labels d'entraînement\n",
    "# et que `val_data` et `val_labels` soient vos données et labels de validation.\n",
    "# epochs représente le nombre de fois où l'ensemble des données d'entraînement est utilisé pour l'apprentissage\n",
    "# batch_size représente le nombre d'échantillons qui seront propagés à travers le réseau à la fois.\n",
    "classifier.fit(x, y, epochs=10, batch_size=32, validation_split=0.2)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
